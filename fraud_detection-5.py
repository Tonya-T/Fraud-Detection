# -*- coding: utf-8 -*-
"""Fraud_Detection.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1dGMBEdcsD3NsHxQp74b4bwCg8c0Wlyzt
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.metrics import accuracy_score,confusion_matrix,f1_score,recall_score,precision_score
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import accuracy_score, classification_report, confusion_matrix
import xgboost as xgb
import pickle
from imblearn.over_sampling import SMOTE
from imblearn.under_sampling import RandomUnderSampler
from imblearn.pipeline import Pipeline
from sklearn.model_selection import train_test_split

data=pd.read_csv('fraudTrain.csv')
data

plt.figure(figsize=(10, 6))
sns.boxplot(x='is_fraud', y='amt', data=data, color='green')
plt.title('Transaction Amounts by Fraud/Non-Fraud')
plt.show()

data['trans_date_trans_time']=pd.to_datetime(data['trans_date_trans_time'])
data['day_of_week']=data['trans_date_trans_time'].dt.dayofweek
data['trans_month']=data['trans_date_trans_time'].dt.month
data['is_weekend'] = data['day_of_week'].isin([5,6]).astype(int)
data['trans_year']=data['trans_date_trans_time'].dt.year
data['trans_hour']=data['trans_date_trans_time'].dt.hour
data['day_of_week'] = data['trans_date_trans_time'].dt.dayofweek
data['is_night'] = data['trans_hour'].isin([21,22,23,0,1,2,3,4,5]).astype(int)
data['is_weekend'] = data['day_of_week'].isin([5,6]).astype(int)
data.drop(columns=['trans_date_trans_time'],inplace=True)
data

data['distance_km'] = np.sqrt(
    (data['lat'] - data['merch_lat'])**2 +
    (data['long'] - data['merch_long'])**2
)

# Calculate transactions per hour for each credit card
data['trans_per_hour'] = data.groupby(['cc_num', 'trans_hour'])['trans_hour'].transform('size')

data['trans_per_hour']

data['mean_amt'] = data.groupby('cc_num')['amt'].transform('mean')
data['mean_amt']

data['relative_amt'] = data['amt'] / data['mean_amt']
data['relative_amt']

# Define a new column to store the count of transactions in the past hour for each credit card
data['trans_in_past_hour'] = data.groupby('cc_num').apply(
    lambda x: x['trans_hour'].rolling(window=2, min_periods=1).count() - 1
).reset_index(level=0, drop=True)

# Display the result
data[['cc_num', 'trans_hour', 'trans_in_past_hour']]

# Calculate the mean transaction amount for each credit card
data['mean_amt_per_card'] = data.groupby('cc_num')['amt'].transform('mean')

# Calculate the difference between each transaction amount and the mean
data['amt_vs_mean'] = data['amt'] - data['mean_amt_per_card']

data

# Calculate the time difference in hours from the previous transaction for each credit card
data['time_diff_from_prev'] = data.groupby('cc_num')['trans_hour'].diff()

# Replace NaN with 0 and convert to integer
data['time_diff_from_prev'] = data['time_diff_from_prev'].fillna(0).astype(int)

# Display the result
data[['cc_num', 'trans_hour', 'time_diff_from_prev']]

# Calculate the fraud rate (probability) for each merchant
merchant_fraud_rate = data.groupby('merchant')['is_fraud'].mean()

# Map this fraud rate back to the original DataFrame as a new feature
data['merchant_fraud_rate'] = data['merchant'].map(merchant_fraud_rate)
data['merchant_fraud_rate']

job_fraud_rate = data.groupby('job')['is_fraud'].mean()

# Map this fraud rate back to the original DataFrame as a new feature
data['job_fraud_rate'] = data['job'].map(job_fraud_rate)

data

high_risk_threshold = 0.05  # e.g., 5%
data['high_risk_merchant'] = data['merchant_fraud_rate'] > high_risk_threshold

data

data = data.drop(["Unnamed: 0",'cc_num',"merchant",'first','last','gender','category','street',
], axis=1)
data

data = data.drop(['city','state', 'trans_num','job','dob'
], axis=1)
data

features=data.drop("is_fraud", axis=1)
features

target=data["is_fraud"]

target = target.dropna()
target

features = features.dropna()
features

print(features.isnull().sum())

# Define SMOTE and undersampling strategy
smote = SMOTE(sampling_strategy=0.1)  # Generate minority samples up to 10% of the majority class
under_sampler = RandomUnderSampler(sampling_strategy=0.5)  # Downsample the majority class

from imblearn.pipeline import Pipeline

# Create pipeline
pipeline = Pipeline(steps=[('smote', smote), ('under', under_sampler)])
X_res, y_res = pipeline.fit_resample(features, target)
print("Number of NaN values in y:", np.isnan(y_res).sum())

print(len(X_res), len(y_res))

X_train, X_test, y_train, y_test=train_test_split(features, target, test_size=0.2,random_state=42)
#train_test_split(features_resampled, target_resampled, random_state= 2023, stratify= target_resampled)
print("X_train:", X_train.shape)
print("X_test:", X_test.shape)
print("y_train:", y_train.shape)
print("y_test:", y_test.shape)

from sklearn.linear_model import LogisticRegression

lr_model=LogisticRegression(random_state=42)

lr_model.fit(X_train,y_train)

lr_predictions=lr_model.predict(X_test)
lr_predictions

lr_accuracy=accuracy_score(y_test, lr_predictions)

lr_accuracy

def evaluate_and_save_model(model, X_train, X_test, y_train, y_test, filename):
  model.fit(X_train, y_train)
  y_pred=model.predict(X_test)
  accuracy=accuracy_score(y_test, y_pred)
  print(f"{model.__class__.__name__}Accuracy:{accuracy:.4f}")
  print(f"\nclassification_report:\n{classification_report(y_test, y_pred)}")
  print("---------------")

  with open(filename, "wb") as file:
     pickle.dump(model, file)


  print(f"Model saved as {filename}")

print("Unique values in y_train:", np.unique(y_train))
print("Unique values in y_test:", np.unique(y_test))

xgb_model=xgb.XGBClassifier(random_state=42)
evaluate_and_save_model(xgb_model, X_train, X_test, y_train, y_test, "xgb_model.pkl")